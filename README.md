# Общие сведения
Научная работа посвящена изучению наноэлектромеханических систем при помощи методов машинного обучения. Подробно о работе можно прочитать в файле "thesis.pdf" в корневой папке.

# Работа разбита на несколько промежуточных этапов:
## Исследование сетки FEM (Mesh Research)
Идея: при слишком разреженной сетке ускоряется процесс сборки датасета, но значительно падает точность вычисляемых параметров, появляются аномалии в данных (резкие всплески, которые не могут быть обоснованы теорией); а при слишком плотной сетке процесс сборки датасета становится очень медленным, при том, что точность вычислений уже не увеличивается.

## Форматирование данных (Data Formatting)
Работа с представлением данных:
- разработаны функции, которые осуществляют поиск аномалий в датасете и их устранение.
- разработаны функции, которые приводят датасет к удобному виду

## Изучение собранных данных (Data Review)
- визуализированы распределения параметров длины, ширины, толщин обоих слоев нанопровода.
- для всех мод колебаний визуализированы распределения резонансной частоты, добротности, эффективной массы и термоупругих потерь
- подобрана нормировка для выходных данных

## Применение методов классического машинного обучения (Classic ML)
В данном разделе содержатся результаты применения следующих моделей:
- `LinearRegressor`
- `RandomForestRegressor`
- `TabNetRegressor`
- `XGBRegressor`
- `XGBRegressor` + `Optuna`
- `XGBRegressor` + sklearn `Cross Validation`
- `XGBRegressor` + custom `Cross Validation`

а также дополнительно визуализируется работа самодельного скейлера данных для контроля качества его работы.

Получены результаты метрик `MSE` и `R2 Score` для тренировочной, валидационной и тестовых выборок для каждой из представленных моделей.

Реализованы функции, при помощи которых можно выводить значения метрик `MSE` и `R2 Score` для каждого из предсказываемых параметров.

Дополнительно, для custom `Cross Validation` реализована визуализация распределения истинных и предсказанных значений каждого из параметров для тренировочной и валидационной части каждого фолда. То есть, на одном графике изображаются `y_true` и `y_pred` как для тренировочной части фолда, так и для валидационной.

## Применение методов нейросетевого машинного обучения (Neural ML)
В данном разделе были применены следующие модели:
#### `Fully Connected Network`, в которую входят несколько слоев `nn.Linear` и `nn.BatchNorm1d`.

#### `Branched Fully Connected Network`. 
Она также состоит из слоев `nn.Linear` и `nn.BatchNorm1d`. 
Сеть делится на три части:
- Общая часть (network_general): в ней осуществляется первичная обработка батча. После нее пайплайн сети расходится на два ветки - короткую и длинную.
- Короткая ветвь (network_short): это один слой `nn.Linear`, который сжимает вектор размера `neck_features` до тензора размера `[batch_size, num_pars_y]`.
- Длинная ветвь  (network_branches): для каждого из параметров создается несколько отдельных слоев `nn.Linear` и `nn.BatchNorm1d`. В конце каждой такой длинной ветви находится слой `nn.Linear(..., out_features=1, ...)`, на выходе которого получается тензор размера `[batch_size, num_pars_y]` (по сути, предсказание одного параметра).

Особенностью сети является алгоритм вычисления ошибки вычислений и алгоритм вычисления конечного результата. Общая ошибка вычислений складывается из суммы ошибок каждой из ветвей (короткой + всех длинных) по формуле:
$$ Loss_{total} = alpha_{long_branch} \; Loss_{long_branch} + alpha_{long_branch} \; Loss_{long_branch} $$
а конечный результат вычисляется как усреднение выходов короткой и длинных ветвей с весами. Все рассматриваемые веса являются гиперпараметрами.

#### `Branched Fully Connected Network`. 
Сеть построена по такому же принципу, как и предыдущая, за исключением метода подсчета ошибки вычислении - тут складывается суммы ошибок каждой из ветвей (короткой + всех длинных) по формуле:
$$ Loss_{total} = \sum_{n = 0}^{N_{branches}} alpha_{n} \; Loss_{n} $$
то есть для каждой из длинных ветвей можно менять свои веса.
